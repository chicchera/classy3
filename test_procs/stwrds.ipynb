{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14e6079-a3ae-464a-9a7e-83aa29f91c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10.8\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545f1583-e867-4b55-97d5-b0d827d0bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/silvio/miniconda3/envs/classy3/prg\n"
     ]
    }
   ],
   "source": [
    "%cd /home/silvio/miniconda3/envs/classy3/prg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a736720f-44d6-45d6-871b-8bffac70bebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fae560a-835b-4ce5-81c4-bfdb7b150f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.data.path = ['/home/silvio/miniconda3/envs/classy3/nltk_data']\n",
    "nltk.data.path.append('/home/silvio/miniconda3/envs/classy3/nltk_data')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332c1e2c-909b-42ac-b931-4062a7b7ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/silvio/nltk_data', '/home/silvio/miniconda3/envs/classy3/nltk_data', '/home/silvio/miniconda3/envs/classy3/share/nltk_data', '/home/silvio/miniconda3/envs/classy3/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/home/silvio/miniconda3/envs/classy3/nltk_data']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd63788-cb5a-4345-aca2-58550a3ed844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.data.path.append('nltk.data.path.append('/home/silvio/miniconda3/envs/classy3/nltk_data)\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ab708cf-7dc8-4d44-98d8-d055ed427f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import linecache\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from rich import print\n",
    "import inspect\n",
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "from utils.file_utils import file_validate, diy_file_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a92198-479f-45fd-80d3-b4cc97a07186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3448c74f-7526-4881-a1f0-91064bee7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stopwords:\n",
    "    def __init__(self):\n",
    "        self._lang = None\n",
    "        self._use_nltk = True\n",
    "        self._stopwords = set()\n",
    "        self._files = []\n",
    "        self._disabled = False\n",
    "\n",
    "    def load_stopwords(self, files, lang=\"es\", nltk=True):\n",
    "        if files:\n",
    "            self._files = files\n",
    "        self._lang = lang\n",
    "        self._use_nltk= nltk\n",
    "\n",
    "        def validate_files_argument(files):\n",
    "            if files is None:\n",
    "                return False  # Argument is None, not valid\n",
    "\n",
    "            if isinstance(files, str):\n",
    "                # If it's a single string, convert it to a list\n",
    "                self._files = [files]\n",
    "                return True\n",
    "\n",
    "            if isinstance(files, list):\n",
    "                # Check if the list is empty\n",
    "                if not files:\n",
    "                    return False\n",
    "\n",
    "                # Check that all elements are of type string\n",
    "                if all(isinstance(item, str) for item in files):\n",
    "                    self._files = files\n",
    "                    return True  # Valid list of strings\n",
    "\n",
    "            return False  # Invalid\n",
    "\n",
    "        def validate_language(val):\n",
    "            if val := val.strip().lower():\n",
    "                if val in [\"es\", \"es_ES\", \"espaañol\"]:\n",
    "                    self.lang = \"spanish\"\n",
    "                elif val in [\"en\", \"en_US\", \"english\"]:\n",
    "                    self.lang = \"english\"\n",
    "                elif val in [\"it\", \"italiano\", \"it_IT\"]:\n",
    "                    self._lang = \"italian\"\n",
    "                else:\n",
    "                    print(\"Language not  set\")\n",
    "                    exit(0)\n",
    "            return True\n",
    "\n",
    "        if not validate_files_argument(files):\n",
    "            self._disabled = True\n",
    "\n",
    "        if not validate_language(lang):\n",
    "            self._disabled = True\n",
    "\n",
    "        if not self._disabled:\n",
    "            new_stopwords = []\n",
    "            for filename in self._files:\n",
    "\n",
    "                success, message = diy_file_validate(filename)\n",
    "                if not success:\n",
    "                    print(message)\n",
    "                    continue\n",
    "\n",
    "                with open(filename, 'r', encoding='utf-8') as f:\n",
    "                    lines = f.readlines()\n",
    "                    for line in lines:\n",
    "                        if word := line.strip():\n",
    "                            new_stopwords.append(word)\n",
    "\n",
    "            self._stopwords = set(new_stopwords)\n",
    "            # TODO: reenable ntlk dictionary\n",
    "            # if self._use_nltk:\n",
    "            #     self._stopwords = self._stopwords | set(stopwords.words(self._lang))\n",
    "\n",
    "    @property\n",
    "    def language(self):\n",
    "        return _lang\n",
    "\n",
    "    @property\n",
    "    def files(self) -> list:\n",
    "        return self._files\n",
    "\n",
    "    @property\n",
    "    def stopwords(self):\n",
    "        return self._stopwords\n",
    "\n",
    "    def count_stopwords(self, text):\n",
    "        text = re.sub(f\"[{string.punctuation}\\n\\r¡¿]\", \"\", text).lower()\n",
    "\n",
    "        # Tokenize the text into words as a set\n",
    "        words = set(nltk.word_tokenize(text, self._lang))\n",
    "        len_words = len(words)\n",
    "\n",
    "        # Remove stopwords using set difference\n",
    "        filtered_words = words - self._stopwords\n",
    "        len_filtered  = len(filtered_words)\n",
    "        print(f\"{len(words)=}\")\n",
    "        print(f\"{len_filtered=}\")\n",
    "\n",
    "        # Reconstruct the text\n",
    "        processed_text = ' '.join(filtered_words)\n",
    "\n",
    "        return len(words) - len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b33cdea-2f3b-488b-bfc1-188b28fefd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sw = Stopwords(\"es\", stopwords_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7d77f4f-036c-4532-8080-d69c6cd07a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = '/home/silvio/miniconda3/envs/classy3/prg/config/'\n",
    "STOPWORD_ES = 'stopwords_es.txt'\n",
    "STOPWORD_RED = 'stopwords_reddit.txt'\n",
    "DICTIONARY = 'new_dic.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1307ac9-2d75-44f0-aa26-e1ec338a389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'/home/silvio/miniconda3/envs/classy3/prg/config/stopwords_es.txt'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'/home/silvio/miniconda3/envs/classy3/prg/config/stopwords_reddit.txt'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'/home/silvio/miniconda3/envs/classy3/prg/config/stopwords_es.txt'\u001b[0m,\n",
       "    \u001b[32m'/home/silvio/miniconda3/envs/classy3/prg/config/stopwords_reddit.txt'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\">/home/silvio/miniconda3/envs/classy3/prg/config/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">new_dic.txt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[35m/home/silvio/miniconda3/envs/classy3/prg/config/\u001b[0m\u001b[95mnew_dic.txt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dictionary = os.path.join(CONFIG_DIR, DICTIONARY)\n",
    "stopwords_files = [os.path.join(CONFIG_DIR, STOPWORD_ES),\n",
    "                  os.path.join(CONFIG_DIR,STOPWORD_RED)]\n",
    "lang = \"es\"\n",
    "print(stopwords_files)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b592e72-ab5a-4ae9-b127-34fd2b07e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sw.files = stopwords_files\n",
    "sw = Stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d502785-ce01-4ad3-8cbb-72c3cbc79222",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.load_stopwords(stopwords_files, lang=(lang), nltk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e887ff1b-6a0e-416f-9861-eea3b8c3abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"\"\"\n",
    "EN ESE TIEMPO REMOTO, yo era muy joven y vivía con mis abuelos en una quinta de paredes\n",
    "blancas de la calle Ocharán, en Miraflores. Estudiaba en San Marcos, Derecho, creo,\n",
    "resignado a ganarme más tarde la vida con una profesión liberal, aunque, en el fondo,\n",
    "me hubiera gustado más llegar a ser un escritor. Tenía un trabajo de título pomposo,\n",
    "sueldo modesto, apropiaciones ilícitas y horario elástico: director de Informaciones de\n",
    "Radio Panamericana. Consistía en recortar las noticias interesantes que aparecían en los\n",
    "diarios y maquillarlas un poco para que se leyeran en los boletines. La redacción a.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22821250-6c3a-4e5d-bcc5-547d9e42c0e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/es.pickle\u001b[0m\n\n  Searched in:\n    - '/home/silvio/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/share/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sw\u001b[39m.\u001b[39;49mcount_stopwords(txt)\n",
      "\u001b[1;32m/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb Cell 16\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X21sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mstring\u001b[39m.\u001b[39mpunctuation\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m¡¿]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, text)\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X21sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Tokenize the text into words as a set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X21sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39;49mword_tokenize(text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X21sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m len_words \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X21sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# Remove stopwords using set difference\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/es.pickle\u001b[0m\n\n  Searched in:\n    - '/home/silvio/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/share/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sw.count_stopwords(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "425ba385-7243-4813-a88f-99c6af8c04fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/es.pickle\u001b[0m\n\n  Searched in:\n    - '/home/silvio/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/share/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(sw\u001b[39m.\u001b[39;49mcount_stopwords(txt))\n",
      "\u001b[1;32m/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb Cell 17\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X22sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m text \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00mstring\u001b[39m.\u001b[39mpunctuation\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\\r\u001b[39;00m\u001b[39m¡¿]\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, text)\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X22sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39m# Tokenize the text into words as a set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X22sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(nltk\u001b[39m.\u001b[39;49mword_tokenize(text, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X22sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m len_words \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(words)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/silvio/miniconda3/envs/classy3/prg/test_procs/stwrds.ipynb#X22sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m \u001b[39m# Remove stopwords using set difference\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[1;32m    132\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/tokenize/__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/data.py:750\u001b[0m, in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[0;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[1;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[1;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[1;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[1;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[0;32m~/miniconda3/envs/classy3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/es.pickle\u001b[0m\n\n  Searched in:\n    - '/home/silvio/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/share/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/silvio/miniconda3/envs/classy3/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "print(sw.count_stopwords(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e38e6-7ecf-4887-9177-a4896073a103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe3dd65-312e-436a-b747-711d2a56e000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
